{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test additive seperability of MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Is there a GPU available: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.config.experimental.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $f(x_0,x_1) =x_0^2+x_1^3$ as our testing function. It has an additive separability property.\n",
    "\n",
    "There are 6 sets of input data with different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0] **2 + x[1] ** 3\n",
    "\n",
    "x_all = np.mgrid[-5:5:0.25, -5:5.25:0.25].reshape(2,-1).T\n",
    "x_all_1 = np.random.normal(0, 2, size=(1640, 2))\n",
    "x_all_2 = np.random.normal(0, 1.5, size=(1640, 2))\n",
    "x_all_3 = np.random.normal(0, 1, size=(1640, 2))\n",
    "x_all_4 = np.random.normal(0, 0.5, size=(1640, 2))\n",
    "x_all_5 = np.random.normal(0, 0.05, size=(1640, 2))\n",
    "\n",
    "y_all=np.array([f(x) for x in x_all])\n",
    "y_all_1=np.array([f(x) for x in x_all_1])\n",
    "y_all_2=np.array([f(x) for x in x_all_2])\n",
    "y_all_3=np.array([f(x) for x in x_all_3])\n",
    "y_all_4=np.array([f(x) for x in x_all_4])\n",
    "y_all_5=np.array([f(x) for x in x_all_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1.  Auto differentiation on MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create MLP and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP model used in the experiments is set with 1 hidden layer and 50 nodes. For calculation convenience, we choose softplus function ($\\ln(1+e^{x})$) as the activation function. The detail of the model is shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 1701.6582 - mae: 26.8414 - mse: 1701.6582 - val_loss: 1543.0806 - val_mae: 30.4718 - val_mse: 1543.0806\n",
      "Epoch 2/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 595.5736 - mae: 18.6498 - mse: 595.5736 - val_loss: 905.1691 - val_mae: 24.2762 - val_mse: 905.1691\n",
      "Epoch 3/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 491.0971 - mae: 18.6858 - mse: 491.0971 - val_loss: 799.8300 - val_mae: 22.7144 - val_mse: 799.8300\n",
      "Epoch 4/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 470.4480 - mae: 18.3095 - mse: 470.4480 - val_loss: 793.4431 - val_mae: 22.6321 - val_mse: 793.4431\n",
      "Epoch 5/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 452.0608 - mae: 18.0016 - mse: 452.0608 - val_loss: 718.2554 - val_mae: 21.6236 - val_mse: 718.2554\n",
      "Epoch 6/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 435.1060 - mae: 17.8103 - mse: 435.1060 - val_loss: 744.7906 - val_mae: 21.8516 - val_mse: 744.7906\n",
      "Epoch 7/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 416.4103 - mae: 17.3160 - mse: 416.4103 - val_loss: 596.4731 - val_mae: 20.0027 - val_mse: 596.4731\n",
      "Epoch 8/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 396.5787 - mae: 16.9429 - mse: 396.5787 - val_loss: 531.9335 - val_mae: 19.1327 - val_mse: 531.9335\n",
      "Epoch 9/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 375.5495 - mae: 16.4725 - mse: 375.5495 - val_loss: 524.6462 - val_mae: 19.1451 - val_mse: 524.6462\n",
      "Epoch 10/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 348.5117 - mae: 15.8445 - mse: 348.5117 - val_loss: 512.2491 - val_mae: 19.0456 - val_mse: 512.2491\n",
      "Epoch 11/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 323.1310 - mae: 15.2271 - mse: 323.1310 - val_loss: 465.9980 - val_mae: 18.3647 - val_mse: 465.9980\n",
      "Epoch 12/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 292.5158 - mae: 14.4467 - mse: 292.5158 - val_loss: 555.1926 - val_mae: 19.6420 - val_mse: 555.1926\n",
      "Epoch 13/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 266.6224 - mae: 13.7831 - mse: 266.6224 - val_loss: 450.8346 - val_mae: 17.7869 - val_mse: 450.8346\n",
      "Epoch 14/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 236.5376 - mae: 12.9465 - mse: 236.5376 - val_loss: 414.9337 - val_mae: 17.1761 - val_mse: 414.9337\n",
      "Epoch 15/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 206.4961 - mae: 12.1024 - mse: 206.4961 - val_loss: 333.5231 - val_mae: 15.4198 - val_mse: 333.5231\n",
      "Epoch 16/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 178.5544 - mae: 11.2090 - mse: 178.5544 - val_loss: 250.6718 - val_mae: 13.1814 - val_mse: 250.6718\n",
      "Epoch 17/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 151.7939 - mae: 10.2321 - mse: 151.7939 - val_loss: 229.5973 - val_mae: 12.8721 - val_mse: 229.5973\n",
      "Epoch 18/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 126.1906 - mae: 9.2441 - mse: 126.1906 - val_loss: 191.3611 - val_mae: 11.8804 - val_mse: 191.3611\n",
      "Epoch 19/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 105.0298 - mae: 8.3942 - mse: 105.0298 - val_loss: 148.9037 - val_mae: 10.3637 - val_mse: 148.9037\n",
      "Epoch 20/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 84.6395 - mae: 7.3741 - mse: 84.6395 - val_loss: 125.6860 - val_mae: 9.1155 - val_mse: 125.6860\n",
      "Epoch 21/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 66.0204 - mae: 6.3576 - mse: 66.0204 - val_loss: 83.8665 - val_mae: 7.2335 - val_mse: 83.8665\n",
      "Epoch 22/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 50.2624 - mae: 5.4250 - mse: 50.2624 - val_loss: 72.9813 - val_mae: 6.8714 - val_mse: 72.9813\n",
      "Epoch 23/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 38.0345 - mae: 4.4549 - mse: 38.0345 - val_loss: 58.7941 - val_mae: 5.8509 - val_mse: 58.7941\n",
      "Epoch 24/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 28.2270 - mae: 3.6527 - mse: 28.2270 - val_loss: 43.3838 - val_mae: 4.8749 - val_mse: 43.3838\n",
      "Epoch 25/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 21.9729 - mae: 3.2328 - mse: 21.9729 - val_loss: 40.9989 - val_mae: 4.6412 - val_mse: 40.9989\n",
      "Epoch 26/5000\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 17.4306 - mae: 2.8819 - mse: 17.4306 - val_loss: 67.6050 - val_mae: 6.1626 - val_mse: 67.6050\n",
      "Epoch 27/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 15.2731 - mae: 2.7184 - mse: 15.2731 - val_loss: 34.5353 - val_mae: 4.3766 - val_mse: 34.5353\n",
      "Epoch 28/5000\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 13.4313 - mae: 2.6281 - mse: 13.4313 - val_loss: 23.5469 - val_mae: 3.7048 - val_mse: 23.5469\n",
      "Epoch 29/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 12.4257 - mae: 2.5230 - mse: 12.4257 - val_loss: 33.8445 - val_mae: 4.3502 - val_mse: 33.8445\n",
      "Epoch 30/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 10.7458 - mae: 2.3853 - mse: 10.7458 - val_loss: 37.6668 - val_mae: 4.7475 - val_mse: 37.6668\n",
      "Epoch 31/5000\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 10.0379 - mae: 2.3000 - mse: 10.0379 - val_loss: 39.9329 - val_mae: 4.9228 - val_mse: 39.9329\n",
      "Epoch 32/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 9.0321 - mae: 2.1867 - mse: 9.0321 - val_loss: 33.4464 - val_mae: 4.5548 - val_mse: 33.4464\n",
      "Epoch 33/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 7.8733 - mae: 2.0596 - mse: 7.8733 - val_loss: 42.9998 - val_mae: 5.2274 - val_mse: 42.9998\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "import timeit\n",
    "from statistics import mean,stdev\n",
    "\n",
    "model = keras.Sequential([\n",
    "layers.Dense(50, activation=tf.keras.activations.softplus, input_shape=[2,]),\n",
    "layers.Dense(1)])\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.01)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=5, verbose=0\n",
    ")\n",
    "\n",
    "model.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['mae', 'mse'])\n",
    "\n",
    "history = model.fit(\n",
    "  x_all, y_all,\n",
    "  epochs=5000, validation_split = 0.2, verbose=1,\n",
    "             callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Calculate $\\partial f / \\partial \\mathbf{x}$ from original MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.18889236 56.8438797 ]\n",
      " [-9.17642593 55.27342606]\n",
      " [-9.13530445 53.22121429]\n",
      " ...\n",
      " [ 5.00059557 47.70395279]\n",
      " [ 4.99287796 48.41611099]\n",
      " [ 4.97862244 48.91444016]]\n"
     ]
    }
   ],
   "source": [
    "x_tensor = tf.convert_to_tensor(x_all)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tensor)\n",
    "    fx = model(x_tensor)  # compute f(x)\n",
    "gradients = tape.gradient(fx, x_tensor).numpy()\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.  Train MLP model for $\\partial f / \\partial x_0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5000\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 2.7900 - mae: 1.1291 - mse: 2.7900 - val_loss: 5.3768 - val_mae: 2.0113 - val_mse: 5.3768\n",
      "Epoch 2/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.5172 - mae: 0.5786 - mse: 0.5172 - val_loss: 2.7982 - val_mae: 1.4763 - val_mse: 2.7982\n",
      "Epoch 3/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.3815 - mae: 0.4926 - mse: 0.3815 - val_loss: 1.5836 - val_mae: 1.0343 - val_mse: 1.5836\n",
      "Epoch 4/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2936 - mae: 0.4281 - mse: 0.2936 - val_loss: 2.1796 - val_mae: 1.2992 - val_mse: 2.1796\n",
      "Epoch 5/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.2094 - mae: 0.3575 - mse: 0.2094 - val_loss: 0.5898 - val_mae: 0.7030 - val_mse: 0.5898\n",
      "Epoch 6/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1821 - mae: 0.3226 - mse: 0.1821 - val_loss: 0.3813 - val_mae: 0.5642 - val_mse: 0.3813\n",
      "Epoch 7/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1669 - mae: 0.3097 - mse: 0.1669 - val_loss: 0.2145 - val_mae: 0.4035 - val_mse: 0.2145\n",
      "Epoch 8/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1378 - mae: 0.2841 - mse: 0.1378 - val_loss: 0.2218 - val_mae: 0.4254 - val_mse: 0.2218\n",
      "Epoch 9/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1314 - mae: 0.2634 - mse: 0.1314 - val_loss: 0.2542 - val_mae: 0.4563 - val_mse: 0.2542\n",
      "Epoch 10/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1331 - mae: 0.2761 - mse: 0.1331 - val_loss: 0.1586 - val_mae: 0.3530 - val_mse: 0.1586\n",
      "Epoch 11/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1289 - mae: 0.2688 - mse: 0.1289 - val_loss: 0.3734 - val_mae: 0.5390 - val_mse: 0.3734\n",
      "Epoch 12/5000\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1118 - mae: 0.2491 - mse: 0.1118 - val_loss: 0.1163 - val_mae: 0.3008 - val_mse: 0.1163\n",
      "Epoch 13/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1325 - mae: 0.2802 - mse: 0.1325 - val_loss: 0.4001 - val_mae: 0.5984 - val_mse: 0.4001\n",
      "Epoch 14/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1014 - mae: 0.2263 - mse: 0.1014 - val_loss: 0.0493 - val_mae: 0.1804 - val_mse: 0.0493\n",
      "Epoch 15/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1238 - mae: 0.2558 - mse: 0.1238 - val_loss: 0.2133 - val_mae: 0.4092 - val_mse: 0.2133\n",
      "Epoch 16/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1106 - mae: 0.2375 - mse: 0.1106 - val_loss: 0.1336 - val_mae: 0.3299 - val_mse: 0.1336\n",
      "Epoch 17/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0956 - mae: 0.2391 - mse: 0.0956 - val_loss: 0.8516 - val_mae: 0.9045 - val_mse: 0.8516\n",
      "Epoch 18/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1106 - mae: 0.2453 - mse: 0.1106 - val_loss: 0.1566 - val_mae: 0.3615 - val_mse: 0.1566\n",
      "Epoch 19/5000\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.1062 - mae: 0.2370 - mse: 0.1062 - val_loss: 1.3423 - val_mae: 1.0890 - val_mse: 1.3423\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "model_d1 = keras.Sequential([\n",
    "layers.Dense(50, activation=tf.keras.activations.relu, input_shape=[2,]),\n",
    "layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.01)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=5, verbose=0\n",
    ") \n",
    "\n",
    "model_d1.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['mae', 'mse'])\n",
    "model_d1.summary()\n",
    "\n",
    "history = model_d1.fit(\n",
    "  x_all, gradients[:,0],\n",
    "  epochs=5000, validation_split = 0.2, verbose=1,\n",
    "             callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Calculate $\\partial^2 f / \\partial^2 x_0 x_1$ from $\\partial f / \\partial x_0$ MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7015242  0.20485801]\n",
      " [0.51277667 0.31509325]\n",
      " [0.51277667 0.31509325]\n",
      " ...\n",
      " [0.48645252 0.06375176]\n",
      " [0.48645252 0.06375176]\n",
      " [0.54313195 0.02645294]]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "x_tensor = tf.convert_to_tensor(x_all)\n",
    "with tf.GradientTape() as tape_d1:\n",
    "    tape_d1.watch(x_tensor)\n",
    "    fx_prime = model_d1(x_tensor)  # compute f(x\n",
    "    \n",
    "gradients_d1 = tape_d1.gradient(fx_prime, x_tensor).numpy()\n",
    "print(gradients_d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation of method 1: differentiation by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train our first MLP model of $f(x_0,x_1)$, instead of using tensorflow auto-differentiation to take the derivative, we can compute the differentiation of the MLP model by hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx0x1_by_hand mean:  0.37636046256784417\n"
     ]
    }
   ],
   "source": [
    "W1=np.array(model.get_weights()[0])\n",
    "W2=np.array(model.get_weights()[2])\n",
    "bias1=np.array(model.get_weights()[1])\n",
    "bias2=np.array(model.get_weights()[3])\n",
    "# use Softplus so that sigma_hat is sigmoid\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def sig_div1(x):\n",
    "    return sig(x)*(1-sig(x))\n",
    "\n",
    "bias1_new=np.repeat(bias1, 1640).reshape((50, 1640))\n",
    "bias2_new=np.repeat(bias2, 1640).reshape((1, 1640))\n",
    "W1_col1=W1[0,:]\n",
    "W1_col2=W1[1,:]\n",
    "\n",
    "y_hand=np.matmul(W2.T,softplus(np.matmul(W1.T,x_all.T)+bias1_new))+bias2_new\n",
    "dx0_by_hand=np.matmul((W2*sig(np.matmul(W1.T,x_all.T))).T,W1_col1)\n",
    "dx1_by_hand=np.matmul((W2*sig(np.matmul(W1.T,x_all.T))).T,W1_col2)\n",
    "\n",
    "dx0x1_by_hand=np.matmul(sig_div1(np.matmul(x_all,W1)),(W2*(W1_col2*W1_col1).reshape((50,1))))\n",
    "dx1x0_by_hand=np.matmul(sig_div1(np.matmul(x_all,W1)),(W2*(W1_col2*W1_col1).reshape((50,1))))\n",
    "\n",
    "print(\"dx0x1_by_hand mean: \",np.mean(abs(dx0x1_by_hand)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2.  Distribute constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Train MLP model on the function $ f(x_0,x_1)$ (same as step 1 in method 1).\n",
    "\n",
    "Step 2. Insert data $x_0, x_1$ and constant $c_0,c_1$ into the function $ f(x_0,x_1)+f(c_0,c_1)-f(c_0,x_1)-f(x_0,c_1)$. The expected value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result mean: 1.4719182\n"
     ]
    }
   ],
   "source": [
    "x0 = [x[0] for x in x_all]\n",
    "x1 = [x[1] for x in x_all]\n",
    "c0=np.random.rand(1640)*5\n",
    "c1=np.random.rand(1640)*5\n",
    "\n",
    "model1=model(x_all,y_all)\n",
    "x0_x1=model.predict(np.column_stack((x0,x1)))\n",
    "x0_c1=model.predict(np.column_stack((x0,c1)))\n",
    "c0_x1=model.predict(np.column_stack((c0,x1)))\n",
    "c0_c1=model.predict(np.column_stack((c0,c1)))\n",
    "result=x0_x1-x0_c1-c0_x1+c0_c1\n",
    "\n",
    "absValues = [abs(number) for number in result]\n",
    "print(\"result mean:\",np.mean(absValues))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
